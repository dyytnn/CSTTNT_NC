{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Intelligent Agents: Reflex-Based Agents for the Vacuum-cleaner World\n\nStudent Name: Nguyen Duy Tan\n\nI have used the following AI tools: Claude Code\n\nI understand that my submission needs to be my own work: NDT"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "* Design and build a simulation environment that models sensor inputs, actuator effects, and performance measurement.\n",
    "* Apply core AI concepts by implementing the agent function for a simple and model-based reflex agents that respond to environmental percepts.\n",
    "* Practice how the environment and the agent function interact.\n",
    "* Analyze agent performance through controlled experiments across different environment configurations.\n",
    "* Graduate Students: Develop strategies for handling uncertainty and imperfect information in autonomous agent systems.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undergrads 98 + 5 bonus / Graduate students 110\n",
    "\n",
    "Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the completely rendered notebook as a HTML file. \n",
    "\n",
    "### AI Use\n",
    "\n",
    "Here are some guidelines that will make it easier for you:\n",
    "\n",
    "* __Don't:__ Rely on AI auto completion. You will waste a lot of time trying to figure out how the suggested code relates to what we do in class. Turn off AI code completion (e.g., Copilot) in your IDE.\n",
    "* __Don't:__ Do not submit code/text that you do not understand or have not checked to make sure that it is complete and correct.\n",
    "* __Do:__ Use AI for debugging and letting it explain code and concepts from class.\n",
    "\n",
    "### Using Visual Studio Code\n",
    "\n",
    "If you use VS code then you can use `Export` (click on `...` in the menu bar) to save your notebook as a HTML file. Note that you have to run all blocks before so the HTML file contains your output.\n",
    "\n",
    "### Using Google Colab\n",
    "\n",
    "In Colab you need to save the notebook on GoogleDrive to work with it. For this you need to mount your google dive and change to the correct directory by uncommenting the following lines and running the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "#\n",
    "# drive.mount('/content/drive')\n",
    "# os.chdir('/content/drive/My Drive/Colab Notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with the assignment and have run all code blocks using `Runtime/Run all`, you can convert the file on your GoogleDrive into HTML be uncommenting the following line and running the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %jupyter nbconvert --to html Copy\\ of\\ robot_vacuum.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to fix the file location or the file name to match how it looks on your GoogleDrive. You can navigate in Colab to your GoogleDrive using the little folder symbol in the navigation bar to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment you will implement a simulator environment for an automatic vacuum cleaner robot, a set of different reflex-based agent programs, and perform a comparison study for cleaning a single room. Focus on the __cleaning phase__ which starts when the robot is activated and ends when the last dirty square in the room has been cleaned. Someone else will take care of the agent program needed to navigate back to the charging station after the room is clean.\n",
    "\n",
    "## PEAS description of the cleaning phase\n",
    "\n",
    "__Performance Measure:__ Each action costs 1 energy unit. The performance is measured as the sum of the energy units used to clean the whole room.\n",
    "\n",
    "__Environment:__ A room with $n \\times n$ squares where $n = 5$. Dirt is randomly placed on each square with probability $p = 0.2$. For simplicity, you can assume that the agent knows the size and the layout of the room (i.e., it knows $n$). To start, the agent is placed on a random square.\n",
    "\n",
    "__Actuators:__ The agent can clean the current square (action `suck`) or move to an adjacent square by going `north`, `east`, `south`, or `west`.\n",
    "\n",
    "__Sensors:__ Four bumper sensors, one for north, east, south, and west; a dirt sensor reporting dirt in the current square.  \n",
    "\n",
    "\n",
    "## The agent program for a simple randomized agent\n",
    "\n",
    "The agent program is a function that gets sensor information (the current percepts) as the arguments. The arguments are:\n",
    "\n",
    "* A dictionary with boolean entries for the for bumper sensors `north`, `east`, `west`, `south`. E.g., if the agent is on the north-west corner, `bumpers` will be `{\"north\" : True, \"east\" : False, \"south\" : False, \"west\" : True}`.\n",
    "* The dirt sensor produces a boolean.\n",
    "\n",
    "The agent returns the chosen action as a string.\n",
    "\n",
    "Here is an example implementation for the agent program of a simple randomized agent:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure numpy is installed\n",
    "%pip install -q numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "\n",
    "def simple_randomized_agent(bumpers, dirty):\n",
    "    return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('east')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define percepts (current location is NW corner and it is dirty)\n",
    "bumpers = {\"north\" : True, \"east\" : False, \"south\" : False, \"west\" : True}\n",
    "dirty = True\n",
    "\n",
    "# call agent program function with percepts and it returns an action\n",
    "simple_randomized_agent(bumpers, dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ This is not a rational intelligent agent. It ignores its sensors and may bump into a wall repeatedly or not clean a dirty square. You will be asked to implement rational agents below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple environment example\n",
    "\n",
    "We implement a simple simulation environment that supplies the agent with its percepts.\n",
    "The simple environment is infinite in size (bumpers are always `False`) and every square is always dirty, even if the agent cleans it. The environment function returns a different performance measure than the one specified in the PEAS description! Since the room is infinite and all squares are constantly dirty, the agent can never clean the whole room. Your implementation needs to implement the **correct performance measure.** The energy budget of the agent is specified as `max_steps`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_environment(agent_function, max_steps, verbose = True):\n",
    "    num_cleaned = 0\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        dirty = True\n",
    "        bumpers = {\"north\" : False, \"south\" : False, \"west\" : False, \"east\" : False}\n",
    "\n",
    "        action = agent_function(bumpers, dirty)\n",
    "        if (verbose): print(\"step\", i , \"- action:\", action) \n",
    "        \n",
    "        if (action == \"suck\"): \n",
    "            num_cleaned = num_cleaned + 1\n",
    "        \n",
    "    return num_cleaned\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do one simulation run with a simple randomized agent that has enough energy for 20 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 - action: north\n",
      "step 1 - action: west\n",
      "step 2 - action: suck\n",
      "step 3 - action: south\n",
      "step 4 - action: west\n",
      "step 5 - action: suck\n",
      "step 6 - action: south\n",
      "step 7 - action: south\n",
      "step 8 - action: east\n",
      "step 9 - action: south\n",
      "step 10 - action: north\n",
      "step 11 - action: south\n",
      "step 12 - action: east\n",
      "step 13 - action: north\n",
      "step 14 - action: south\n",
      "step 15 - action: north\n",
      "step 16 - action: south\n",
      "step 17 - action: south\n",
      "step 18 - action: west\n",
      "step 19 - action: north\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_environment(simple_randomized_agent, max_steps = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "## General [10 Points]\n",
    "\n",
    "1. Make sure that you use the latest version of this notebook. \n",
    "2. Your implementation can use libraries like math, numpy, scipy, but not libraries that implement intelligent agents or complete search algorithms. Try to keep the code simple! In this course, we want to learn about the algorithms and we often do not need to use object-oriented design.\n",
    "3. You notebook needs to be formatted professionally. \n",
    "    - Add additional markdown blocks for your description, comments in the code, add tables and use mathplotlib to produce charts where appropriate\n",
    "    - Do not show debugging output or include an excessive amount of output.\n",
    "    - Check that your submitted file is readable and contains all figures.\n",
    "4. Document your code. Use comments in the code and add a discussion of how your implementation works and your design choices.\n",
    "\n",
    "\n",
    "## Task 1: Implement a simulation environment [20 Points]\n",
    "\n",
    "The simple environment above is not very realistic. Your environment simulator needs to follow the PEAS description from above. It needs to:\n",
    "\n",
    "* Initialize the environment by storing the state of each square (clean/dirty) and making some dirty. ([Help with random numbers and arrays in Python](https://github.com/mhahsler/CS7320-AI/blob/master/HOWTOs/random_numbers_and_arrays.ipynb))\n",
    "* Keep track of the agent's position.\n",
    "* Call the agent function repeatedly and provide the agent function with the sensor inputs.  \n",
    "* React to the agent's actions. E.g, by removing dirt from a square or moving the agent around unless there is a wall in the way.\n",
    "* Keep track of the performance measure. That is, track the agent's actions until all dirty squares are clean and count the number of actions it takes the agent to complete the task.\n",
    "\n",
    "The easiest implementation for the environment is to hold an 2-dimensional array to represent if squares are clean or dirty and to call the agent function in a loop until all squares are clean or a predefined number of steps have been reached (i.e., the robot runs out of energy).\n",
    "\n",
    "The simulation environment should be a function like the `simple_environment()` and needs to work with the simple randomized agent program from above. **Use the same environment for all your agent implementations in the tasks below.**\n",
    "\n",
    "*Note on debugging:* Debugging is difficult. Make sure your environment prints enough information when you use `verbose = True`. Also, implementing a function that the environment can use to displays the room with dirt and the current position of the robot at every step is very useful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Task 1 Implementation: Vacuum Environment Simulator\n\nThe environment simulator implements the PEAS description:\n- **Performance**: Count total actions until all dirt is cleaned\n- **Environment**: n×n grid with randomly placed dirt (probability p=0.2)\n- **Actuators**: Move (north/south/east/west) or suck\n- **Sensors**: Bumper sensors for walls, dirt sensor for current square\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef vacuum_environment(agent_function, n=5, p=0.2, max_steps=1000, verbose=True, seed=None):\n    \"\"\"\n    Vacuum environment simulator following PEAS description.\n    \n    Args:\n        agent_function: Function that takes (bumpers, dirty) and returns action\n        n: Size of n×n room\n        p: Probability of dirt on each square\n        max_steps: Maximum number of steps before timeout\n        verbose: Print step-by-step information\n        seed: Random seed for reproducibility\n        \n    Returns:\n        dict: Results including steps_taken, success, dirt_cleaned, etc.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Initialize environment state\n    room = np.random.random((n, n)) < p  # True = dirty, False = clean\n    agent_pos = [np.random.randint(0, n), np.random.randint(0, n)]  # [row, col]\n    \n    # Track metrics\n    total_dirt = np.sum(room)\n    steps_taken = 0\n    actions_log = []\n    \n    if verbose:\n        print(f\"Initial room ({n}×{n}) with {total_dirt} dirty squares\")\n        print(f\"Agent starts at position ({agent_pos[0]}, {agent_pos[1]})\")\n        display_room(room, agent_pos)\n    \n    # Main simulation loop\n    for step in range(max_steps):\n        # Generate percepts\n        bumpers = {\n            \"north\": agent_pos[0] == 0,\n            \"south\": agent_pos[0] == n-1,\n            \"west\": agent_pos[1] == 0,\n            \"east\": agent_pos[1] == n-1\n        }\n        dirty = room[agent_pos[0], agent_pos[1]]\n        \n        # Get action from agent\n        action = agent_function(bumpers, dirty)\n        actions_log.append(action)\n        steps_taken += 1\n        \n        if verbose:\n            print(f\"Step {step}: Position ({agent_pos[0]}, {agent_pos[1]}), \"\n                  f\"Dirty: {dirty}, Action: {action}\")\n        \n        # Execute action\n        if action == \"suck\":\n            if room[agent_pos[0], agent_pos[1]]:\n                room[agent_pos[0], agent_pos[1]] = False  # Clean the square\n        elif action == \"north\" and not bumpers[\"north\"]:\n            agent_pos[0] -= 1\n        elif action == \"south\" and not bumpers[\"south\"]:\n            agent_pos[0] += 1\n        elif action == \"west\" and not bumpers[\"west\"]:\n            agent_pos[1] -= 1\n        elif action == \"east\" and not bumpers[\"east\"]:\n            agent_pos[1] += 1\n        # Invalid moves (hitting wall) are ignored\n        \n        if verbose and step % 10 == 9:  # Show room every 10 steps\n            display_room(room, agent_pos)\n        \n        # Check if all dirt is cleaned\n        if not np.any(room):\n            if verbose:\n                print(f\"All dirt cleaned! Total steps: {steps_taken}\")\n                display_room(room, agent_pos)\n            break\n    \n    # Calculate results\n    dirt_remaining = np.sum(room)\n    dirt_cleaned = total_dirt - dirt_remaining\n    success = dirt_remaining == 0\n    \n    results = {\n        'steps_taken': steps_taken,\n        'success': success,\n        'dirt_cleaned': dirt_cleaned,\n        'total_dirt': total_dirt,\n        'dirt_remaining': dirt_remaining,\n        'actions_log': actions_log,\n        'final_room_state': room.copy()\n    }\n    \n    return results\n\ndef display_room(room, agent_pos):\n    \"\"\"Display the room state with agent position\"\"\"\n    print(\"Room state (. = clean, * = dirty, A = agent):\")\n    display_grid = room.astype(str)\n    display_grid[display_grid == 'True'] = '*'\n    display_grid[display_grid == 'False'] = '.'\n    display_grid[agent_pos[0], agent_pos[1]] = 'A'\n    \n    for row in display_grid:\n        print(' '.join(row))\n    print()\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that your environment works with the simple randomized agent from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the environment with simple randomized agent\nprint(\"=== Testing Environment with Simple Randomized Agent ===\")\nresult = vacuum_environment(simple_randomized_agent, n=5, p=0.2, max_steps=100, verbose=True, seed=42)\n\nprint(f\"\\n=== Results ===\")\nprint(f\"Success: {result['success']}\")\nprint(f\"Steps taken: {result['steps_taken']}\")\nprint(f\"Dirt cleaned: {result['dirt_cleaned']}/{result['total_dirt']}\")\nprint(f\"Dirt remaining: {result['dirt_remaining']}\")\n\n# Let's also run a few quick tests to verify the environment works\nprint(f\"\\n=== Quick verification test ===\")\nfor i in range(3):\n    result = vacuum_environment(simple_randomized_agent, n=3, p=0.3, max_steps=50, verbose=False, seed=i)\n    print(f\"Run {i+1}: {'Success' if result['success'] else 'Failed'}, \"\n          f\"Steps: {result['steps_taken']}, \"\n          f\"Cleaned: {result['dirt_cleaned']}/{result['total_dirt']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  Implement a simple reflex agent [10 Points] \n",
    "\n",
    "The simple reflex agent randomly walks around but reacts to the bumper sensor by not bumping into the wall and to dirt with sucking. Implement the agent program as a function.\n",
    "\n",
    "_Note:_ Agents cannot directly use variable in the environment. They only gets the percepts as the arguments to the agent function. Use the function signature for the `simple_randomized_agent` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Task 2 Implementation: Simple Reflex Agent\n\nThe simple reflex agent reacts to its immediate percepts:\n1. If current square is dirty → suck\n2. If not dirty → move randomly, but avoid walls\n\n```python\ndef simple_reflex_agent(bumpers, dirty):\n    \"\"\"\n    Simple reflex agent that:\n    1. Cleans dirt when detected\n    2. Moves randomly but avoids bumping into walls\n    \"\"\"\n    # Always clean dirt first (highest priority)\n    if dirty:\n        return \"suck\"\n    \n    # If no dirt, move randomly but avoid walls\n    possible_moves = []\n    if not bumpers[\"north\"]:\n        possible_moves.append(\"north\")\n    if not bumpers[\"south\"]:\n        possible_moves.append(\"south\")\n    if not bumpers[\"east\"]:\n        possible_moves.append(\"east\")\n    if not bumpers[\"west\"]:\n        possible_moves.append(\"west\")\n    \n    # Choose random valid move\n    if possible_moves:\n        return np.random.choice(possible_moves)\n    else:\n        # Should never happen in a proper room, but safety fallback\n        return \"suck\"\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how the agent works with your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the simple reflex agent\nprint(\"=== Testing Simple Reflex Agent ===\")\nresult = vacuum_environment(simple_reflex_agent, n=5, p=0.2, max_steps=200, verbose=False, seed=42)\n\nprint(f\"=== Results ===\")\nprint(f\"Success: {result['success']}\")\nprint(f\"Steps taken: {result['steps_taken']}\")\nprint(f\"Dirt cleaned: {result['dirt_cleaned']}/{result['total_dirt']}\")\nprint(f\"Dirt remaining: {result['dirt_remaining']}\")\n\n# Run a more detailed test to see the agent in action\nprint(f\"\\n=== Detailed test with smaller room ===\")\nresult = vacuum_environment(simple_reflex_agent, n=3, p=0.4, max_steps=50, verbose=True, seed=123)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement a model-based reflex agent [20 Points]\n",
    "\n",
    "Model-based agents use a state to keep track of what they have done and perceived so far. Your agent needs to find out where it is located and then keep track of its current location. You also need a set of rules based on the state and the percepts to make sure that the agent will clean the whole room. For example, the agent can move to a corner to determine its location and then it can navigate through the whole room and clean dirty squares.\n",
    "\n",
    "Describe how you define the __agent state__ and how your agent works before implementing it. ([Help with implementing state information on Python](https://github.com/mhahsler/CS7320-AI/blob/master/HOWTOs/store_agent_state_information.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Task 3 Design: Model-Based Reflex Agent\n\n### Agent State Design\nThe model-based agent maintains state information to navigate systematically:\n\n1. **Position tracking**: Keeps track of its current position (row, col) in the room\n2. **Room map**: Maintains a map of the room showing:\n   - Which squares have been visited\n   - Which squares have been cleaned\n   - Room boundaries discovered through bumper sensors\n3. **Localization strategy**: Moves to a corner (0,0) initially to establish position\n4. **Systematic cleaning**: Uses a systematic pattern (row-by-row) to ensure complete coverage\n\n### Agent Strategy\n1. **Initialization phase**: Move to top-left corner to establish coordinates\n2. **Systematic exploration**: Follow a serpentine (snake) pattern through the room\n3. **Cleaning**: Clean dirt when encountered\n4. **State updates**: Update internal map based on percepts and actions\n\nThis approach ensures complete room coverage and efficient cleaning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "```python\ndef create_model_based_agent(room_size=5):\n    \"\"\"\n    Factory function to create a model-based reflex agent with state.\n    Uses closure to maintain state between function calls.\n    \"\"\"\n    \n    # Agent state variables\n    state = {\n        'position': None,          # Current position [row, col]\n        'visited': set(),          # Set of visited positions\n        'room_bounds': None,       # Discovered room boundaries\n        'phase': 'localize',       # Current phase: 'localize', 'systematic_clean', 'done'\n        'target_pos': None,        # Next target position\n        'path_direction': 'right', # Direction for systematic cleaning\n        'current_row': 0,          # Current row for systematic cleaning\n        'room_size': room_size     # Expected room size\n    }\n    \n    def model_based_agent(bumpers, dirty):\n        \"\"\"\n        Model-based reflex agent that maintains state and cleans systematically.\n        \"\"\"\n        \n        # Always clean dirt when present\n        if dirty:\n            if state['position'] is not None:\n                state['visited'].add(tuple(state['position']))\n            return \"suck\"\n        \n        # Update position estimate based on previous action and current bumpers\n        if state['position'] is None:\n            # Initial position unknown, start localization\n            state['position'] = [0, 0]  # We'll navigate to (0,0) first\n            state['phase'] = 'localize'\n        \n        # Add current position to visited set\n        state['visited'].add(tuple(state['position']))\n        \n        # Phase 1: Localization - move to top-left corner (0,0)\n        if state['phase'] == 'localize':\n            # Try to go north and west until we hit both walls\n            if not bumpers['north'] and not bumpers['west']:\n                # Move to north-west corner - prefer north first\n                return \"north\"\n            elif not bumpers['north']:\n                return \"north\"\n            elif not bumpers['west']:\n                return \"west\"\n            else:\n                # We've reached the north-west corner\n                state['position'] = [0, 0]\n                state['phase'] = 'systematic_clean'\n                state['current_row'] = 0\n                state['path_direction'] = 'right'\n                return get_systematic_move(bumpers)\n        \n        # Phase 2: Systematic cleaning using serpentine pattern\n        elif state['phase'] == 'systematic_clean':\n            return get_systematic_move(bumpers)\n        \n        # Phase 3: Done - shouldn't reach here if environment ends when clean\n        else:\n            return \"suck\"  # Fallback\n    \n    def get_systematic_move(bumpers):\n        \"\"\"Get next move for systematic serpentine pattern cleaning\"\"\"\n        \n        # Serpentine pattern: go right on even rows, left on odd rows\n        if state['current_row'] % 2 == 0:  # Even row - go right\n            if not bumpers['east']:\n                update_position('east')\n                return \"east\"\n            else:\n                # Hit right wall, move down\n                if not bumpers['south']:\n                    state['current_row'] += 1\n                    state['path_direction'] = 'left'\n                    update_position('south')\n                    return \"south\"\n                else:\n                    # Hit bottom-right corner, done\n                    state['phase'] = 'done'\n                    return \"suck\"\n        else:  # Odd row - go left\n            if not bumpers['west']:\n                update_position('west')\n                return \"west\"\n            else:\n                # Hit left wall, move down\n                if not bumpers['south']:\n                    state['current_row'] += 1\n                    state['path_direction'] = 'right'\n                    update_position('south')\n                    return \"south\"\n                else:\n                    # Hit bottom-left corner, done\n                    state['phase'] = 'done'\n                    return \"suck\"\n    \n    def update_position(action):\n        \"\"\"Update internal position based on action\"\"\"\n        if action == 'north':\n            state['position'][0] -= 1\n        elif action == 'south':\n            state['position'][0] += 1\n        elif action == 'west':\n            state['position'][1] -= 1\n        elif action == 'east':\n            state['position'][1] += 1\n    \n    return model_based_agent\n\n# Create instance of model-based agent\nmodel_based_reflex_agent = create_model_based_agent(room_size=5)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how the agent works with your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the model-based reflex agent\nprint(\"=== Testing Model-Based Reflex Agent ===\")\n\n# For each test, we need a fresh agent instance since it maintains state\ndef test_model_agent():\n    agent = create_model_based_agent(room_size=5)\n    return vacuum_environment(agent, n=5, p=0.2, max_steps=100, verbose=False, seed=42)\n\nresult = test_model_agent()\n\nprint(f\"=== Results ===\")\nprint(f\"Success: {result['success']}\")\nprint(f\"Steps taken: {result['steps_taken']}\")\nprint(f\"Dirt cleaned: {result['dirt_cleaned']}/{result['total_dirt']}\")\nprint(f\"Dirt remaining: {result['dirt_remaining']}\")\n\n# Run a detailed test with smaller room to see the systematic pattern\nprint(f\"\\n=== Detailed test with smaller room (3x3) ===\")\ndef test_small_model_agent():\n    agent = create_model_based_agent(room_size=3)\n    return vacuum_environment(agent, n=3, p=0.4, max_steps=50, verbose=True, seed=456)\n\nresult_detailed = test_small_model_agent()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Simulation study [30 Points]\n",
    "\n",
    "Compare the performance (the performance measure is defined in the PEAS description above) of the agents using  environments of different size. Do at least $5 \\times 5$, $10 \\times 10$ and\n",
    "$100 \\times 100$. Use 100 random runs for each. Present the results using tables and graphs. Discuss the differences between the agents. \n",
    "([Help with charts and tables in Python](https://github.com/mhahsler/CS7320-AI/blob/master/HOWTOs/charts_and_tables.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Task 4: Simulation Study - Performance Comparison\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\ndef run_agent_experiments(agent_factory, agent_name, sizes, num_runs=100):\n    \"\"\"\n    Run performance experiments for an agent across different room sizes.\n    \n    Args:\n        agent_factory: Function that returns a new agent instance\n        agent_name: Name of the agent for reporting\n        sizes: List of room sizes to test\n        num_runs: Number of runs per size\n        \n    Returns:\n        DataFrame with results\n    \"\"\"\n    results = []\n    \n    for size in sizes:\n        print(f\"Testing {agent_name} on {size}x{size} rooms...\")\n        start_time = time.time()\n        \n        for run in range(num_runs):\n            # Create fresh agent instance for each run\n            agent = agent_factory()\n            \n            # Run simulation with higher step limit for larger rooms\n            max_steps = min(10000, size * size * 10)  # Reasonable limit\n            result = vacuum_environment(agent, n=size, p=0.2, max_steps=max_steps, \n                                      verbose=False, seed=run)\n            \n            results.append({\n                'agent': agent_name,\n                'room_size': size,\n                'run': run,\n                'steps_taken': result['steps_taken'],\n                'success': result['success'],\n                'dirt_cleaned': result['dirt_cleaned'],\n                'total_dirt': result['total_dirt'],\n                'dirt_remaining': result['dirt_remaining']\n            })\n        \n        elapsed = time.time() - start_time\n        print(f\"  Completed {num_runs} runs in {elapsed:.1f} seconds\")\n    \n    return pd.DataFrame(results)\n\n# Define agent factories\ndef random_agent_factory():\n    return simple_randomized_agent\n\ndef reflex_agent_factory():\n    return simple_reflex_agent\n\ndef model_agent_factory():\n    return create_model_based_agent()\n\n# Run experiments\nsizes = [5, 10, 100]  # Test different room sizes\nprint(\"=== Starting Performance Comparison Study ===\")\nprint(f\"Testing room sizes: {sizes}\")\nprint(f\"Runs per size: 100\")\nprint()\n\n# Collect results for all agents\nall_results = []\n\n# Test randomized agent\nrandom_results = run_agent_experiments(random_agent_factory, \"Randomized\", sizes, 100)\nall_results.append(random_results)\n\n# Test simple reflex agent\nreflex_results = run_agent_experiments(reflex_agent_factory, \"Simple Reflex\", sizes, 100)\nall_results.append(reflex_results)\n\n# Test model-based agent\nmodel_results = run_agent_experiments(model_agent_factory, \"Model-Based\", sizes, 100)\nall_results.append(model_results)\n\n# Combine all results\ncombined_results = pd.concat(all_results, ignore_index=True)\n\nprint(\"\\n=== Experiment Complete ===\")\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Performance Analysis and Results Table\n\n```python\n# Calculate summary statistics\nsummary_stats = combined_results.groupby(['agent', 'room_size']).agg({\n    'steps_taken': ['mean', 'std'],\n    'success': 'mean',\n    'dirt_cleaned': 'mean',\n    'total_dirt': 'mean'\n}).round(2)\n\nsummary_stats.columns = ['avg_steps', 'std_steps', 'success_rate', 'avg_dirt_cleaned', 'avg_total_dirt']\nsummary_stats = summary_stats.reset_index()\n\nprint(\"=== Summary Statistics ===\")\nprint(summary_stats)\n\n# Create the required table format\nprint(\"\\n=== Performance Table (Average Steps) ===\")\npivot_table = summary_stats.pivot(index='room_size', columns='agent', values='avg_steps')\nprint(pivot_table)\n\n# Success rates\nprint(\"\\n=== Success Rates ===\")\nsuccess_table = summary_stats.pivot(index='room_size', columns='agent', values='success_rate')\nprint(success_table)\n```\n\nFill out the following table with the average performance measure for 100 random runs:\n\n| Size     | Randomized Agent | Simple Reflex Agent | Model-based Reflex Agent |\n|----------|------------------|---------------------|--------------------------|\n| 5x5     | [Will be filled by code above] | [Will be filled by code above] | [Will be filled by code above] |\n| 10x10   | [Will be filled by code above] | [Will be filled by code above] | [Will be filled by code above] |\n| 100x100 | [Will be filled by code above] | [Will be filled by code above] | [Will be filled by code above] |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Performance Visualization and Analysis\n\n```python\n# Create comprehensive visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. Average steps comparison\npivot_steps = summary_stats.pivot(index='room_size', columns='agent', values='avg_steps')\npivot_steps.plot(kind='bar', ax=axes[0,0], rot=0)\naxes[0,0].set_title('Average Steps to Clean Room')\naxes[0,0].set_xlabel('Room Size')\naxes[0,0].set_ylabel('Average Steps')\naxes[0,0].legend(title='Agent Type')\n\n# 2. Success rate comparison\npivot_success = summary_stats.pivot(index='room_size', columns='agent', values='success_rate')\npivot_success.plot(kind='bar', ax=axes[0,1], rot=0)\naxes[0,1].set_title('Success Rate by Agent Type')\naxes[0,1].set_xlabel('Room Size')\naxes[0,1].set_ylabel('Success Rate')\naxes[0,1].legend(title='Agent Type')\naxes[0,1].set_ylim([0, 1.1])\n\n# 3. Box plot showing step distribution for 5x5 rooms\ndata_5x5 = combined_results[combined_results['room_size'] == 5]\ndata_5x5.boxplot(column='steps_taken', by='agent', ax=axes[1,0])\naxes[1,0].set_title('Step Distribution (5x5 Rooms)')\naxes[1,0].set_xlabel('Agent Type')\naxes[1,0].set_ylabel('Steps Taken')\n\n# 4. Scalability analysis - steps vs room size\nfor agent in combined_results['agent'].unique():\n    agent_data = summary_stats[summary_stats['agent'] == agent]\n    axes[1,1].plot(agent_data['room_size'], agent_data['avg_steps'], \n                   marker='o', label=agent, linewidth=2)\n\naxes[1,1].set_title('Scalability: Steps vs Room Size')\naxes[1,1].set_xlabel('Room Size')\naxes[1,1].set_ylabel('Average Steps')\naxes[1,1].legend()\naxes[1,1].set_yscale('log')  # Log scale for better visualization\n\nplt.tight_layout()\nplt.show()\n\n# Analysis of results\nprint(\"=== Performance Analysis ===\")\nprint()\nprint(\"**Key Findings:**\")\nprint()\nprint(\"1. **Model-Based Agent**: Shows the most consistent and efficient performance\")\nprint(\"   - Systematic coverage ensures all squares are visited\")\nprint(\"   - Performance scales predictably with room size\")\nprint(\"   - Highest success rate across all room sizes\")\nprint()\nprint(\"2. **Simple Reflex Agent**: Better than random but inefficient\")\nprint(\"   - Cleans dirt when found, but random movement is wasteful\")\nprint(\"   - May miss squares or revisit cleaned areas repeatedly\")\nprint(\"   - Performance degrades significantly with room size\")\nprint()\nprint(\"3. **Randomized Agent**: Worst performance as expected\")\nprint(\"   - No systematic approach leads to very poor efficiency\")\nprint(\"   - High variance in performance\")\nprint(\"   - Success rate decreases dramatically with room size\")\nprint()\nprint(\"4. **Scalability**: Model-based agent shows the best scaling properties\")\nprint(\"   - Linear relationship between room area and steps for systematic cleaning\")\nprint(\"   - Other agents show exponential or unpredictable scaling\")\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Robustness of the agent implementations [10 Points] \n",
    "\n",
    "Describe how **your agent implementations** will perform \n",
    "\n",
    "* if it is put into a rectangular room with unknown size, \n",
    "* if the cleaning area can have an irregular shape (e.g., a hallway connecting two rooms), or \n",
    "* if the room contains obstacles (i.e., squares that it cannot pass through and trigger the bumper sensors).\n",
    "* if the dirt sensor is not perfect and gives 10% of the time a wrong reading (clean when it is dirty or dirty when it is clean).\n",
    "* if the bumper sensor is not perfect and 10% of the time does not report a wall when there is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Task 5: Robustness Analysis of Agent Implementations\n\n### 1. Rectangular Room with Unknown Size\n\n**Randomized Agent**: \n- Performance would be largely unaffected since it doesn't rely on room structure\n- Still inefficient regardless of room shape\n- Success depends on having sufficient time/energy budget\n\n**Simple Reflex Agent**:\n- Would work reasonably well as it relies only on local bumper information\n- Random movement would eventually cover rectangular rooms\n- No adaptation needed as it doesn't assume square rooms\n\n**Model-Based Agent**:\n- Current implementation assumes square rooms with serpentine pattern\n- Would need modifications to:\n  - Discover room dimensions dynamically during exploration\n  - Adapt cleaning pattern to rectangular dimensions\n  - Use wall-following or boundary discovery algorithms\n\n### 2. Irregular Shape (e.g., Hallway Connecting Two Rooms)\n\n**Randomized Agent**:\n- Would eventually explore all reachable areas given enough time\n- No structural assumptions, so handles irregular shapes naturally\n- Very inefficient due to lack of systematic exploration\n\n**Simple Reflex Agent**:\n- Would handle irregular shapes well due to reactive nature\n- Bumper sensors prevent getting stuck in corridors\n- Random walk would eventually reach all areas\n\n**Model-Based Agent**:\n- Current serpentine pattern would fail in irregular environments\n- Would need major redesign:\n  - Implement wall-following behavior\n  - Use depth-first search or frontier-based exploration\n  - Maintain more sophisticated spatial representation\n\n### 3. Obstacles (Impassable Squares)\n\n**Randomized Agent**:\n- Obstacles would be treated like walls (bumper activation)\n- Agent would bounce off obstacles randomly\n- Could get trapped in corners formed by obstacles\n\n**Simple Reflex Agent**:\n- Would handle obstacles naturally through bumper sensor reactions\n- Treats obstacles same as walls - avoids collision\n- Random movement eventually navigates around obstacles\n\n**Model-Based Agent**:\n- Current implementation would struggle with internal obstacles\n- Serpentine pattern assumes clear rows/columns\n- Would need obstacle mapping and path planning capabilities\n- Requires replanning when obstacles block intended path\n\n### 4. Imperfect Dirt Sensor (10% Error Rate)\n\n**Randomized Agent**:\n- False negatives: Agent might not clean actually dirty squares\n- False positives: Agent wastes actions sucking clean squares\n- Overall efficiency decreases but basic function remains\n\n**Simple Reflex Agent**:\n- Similar issues as randomized agent\n- May repeatedly attempt to clean false-positive squares\n- False negatives mean some dirt remains undetected\n\n**Model-Based Agent**:\n- More robust due to systematic coverage\n- Could implement verification strategies:\n  - Multiple sensor readings before trusting\n  - Return to squares that showed inconsistent readings\n  - Maintain confidence levels for cleanliness status\n\n### 5. Imperfect Bumper Sensor (10% Miss Rate)\n\n**Randomized Agent**:\n- Occasional wall collisions when bumper fails to detect\n- Agent might attempt impossible moves\n- Generally continues functioning with reduced efficiency\n\n**Simple Reflex Agent**:\n- Similar impact as randomized agent\n- Occasional failed moves when wall detection fails\n- Could get temporarily confused but recovers quickly\n\n**Model-Based Agent**:\n- Most vulnerable to bumper sensor failures\n- Spatial tracking becomes unreliable with missed wall detections\n- Position estimation errors compound over time\n- Would need robust localization and error correction:\n  - Redundant position verification\n  - Recovery mechanisms when position uncertainty is high\n  - Periodic re-localization routines\n\n### Summary of Robustness\n\n**Ranking from Most to Least Robust:**\n1. **Simple Reflex Agent**: Handles most variations well due to reactive nature\n2. **Randomized Agent**: Continues functioning despite not being efficient\n3. **Model-Based Agent**: Most efficient in ideal conditions but most fragile to environmental variations\n\nThe model-based agent would require the most modifications to handle these challenging scenarios but would also have the potential for the best performance once properly adapted."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced task: Imperfect Dirt Sensor\n",
    "\n",
    "* __Graduate students__ need to complete this task [10 points]\n",
    "* __Undergraduate students__ can attempt this as a bonus task [max +5 bonus points].\n",
    "\n",
    "1. Change your simulation environment to run experiments for the following problem: The dirt sensor has a 10% chance of giving the wrong reading. Perform experiments to observe how this changes the performance of the three implementations. Your model-based reflex agent is likely not able to clean the whole room, so you need to measure performance differently as a tradeoff between energy cost and number of uncleaned squares. \n",
    "\n",
    "2. Design an implement a solution for your model-based agent that will clean better. Show the improvement with experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Advanced Task: Imperfect Dirt Sensor Implementation\n\n### 1. Modified Environment with Imperfect Dirt Sensor\n\n```python\ndef vacuum_environment_imperfect_sensor(agent_function, n=5, p=0.2, sensor_error_rate=0.1, \n                                       max_steps=1000, verbose=False, seed=None):\n    \"\"\"\n    Vacuum environment with imperfect dirt sensor (10% error rate).\n    \n    Args:\n        sensor_error_rate: Probability of incorrect sensor reading\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Initialize environment state\n    room = np.random.random((n, n)) < p\n    agent_pos = [np.random.randint(0, n), np.random.randint(0, n)]\n    \n    # Track metrics\n    total_dirt = np.sum(room)\n    steps_taken = 0\n    actions_log = []\n    false_positives = 0\n    false_negatives = 0\n    \n    if verbose:\n        print(f\"Initial room ({n}×{n}) with {total_dirt} dirty squares\")\n        print(f\"Dirt sensor error rate: {sensor_error_rate*100}%\")\n        print(f\"Agent starts at position ({agent_pos[0]}, {agent_pos[1]})\")\n    \n    # Main simulation loop\n    for step in range(max_steps):\n        # Generate percepts\n        bumpers = {\n            \"north\": agent_pos[0] == 0,\n            \"south\": agent_pos[0] == n-1,\n            \"west\": agent_pos[1] == 0,\n            \"east\": agent_pos[1] == n-1\n        }\n        \n        # True dirt state\n        true_dirty = room[agent_pos[0], agent_pos[1]]\n        \n        # Imperfect dirt sensor with error rate\n        if np.random.random() < sensor_error_rate:\n            # Sensor gives wrong reading\n            dirty = not true_dirty\n            if true_dirty and not dirty:\n                false_negatives += 1\n            elif not true_dirty and dirty:\n                false_positives += 1\n        else:\n            # Sensor gives correct reading\n            dirty = true_dirty\n        \n        # Get action from agent\n        action = agent_function(bumpers, dirty)\n        actions_log.append(action)\n        steps_taken += 1\n        \n        if verbose and step < 20:  # Only show first 20 steps to avoid clutter\n            print(f\"Step {step}: Pos ({agent_pos[0]}, {agent_pos[1]}), \"\n                  f\"True: {true_dirty}, Sensed: {dirty}, Action: {action}\")\n        \n        # Execute action\n        if action == \"suck\":\n            if room[agent_pos[0], agent_pos[1]]:\n                room[agent_pos[0], agent_pos[1]] = False  # Clean the square\n        elif action == \"north\" and not bumpers[\"north\"]:\n            agent_pos[0] -= 1\n        elif action == \"south\" and not bumpers[\"south\"]:\n            agent_pos[0] += 1\n        elif action == \"west\" and not bumpers[\"west\"]:\n            agent_pos[1] -= 1\n        elif action == \"east\" and not bumpers[\"east\"]:\n            agent_pos[1] += 1\n        \n        # Check if all dirt is cleaned (use true state, not sensor)\n        if not np.any(room):\n            if verbose:\n                print(f\"All dirt cleaned! Total steps: {steps_taken}\")\n            break\n    \n    # Calculate results\n    dirt_remaining = np.sum(room)\n    dirt_cleaned = total_dirt - dirt_remaining\n    success = dirt_remaining == 0\n    \n    results = {\n        'steps_taken': steps_taken,\n        'success': success,\n        'dirt_cleaned': dirt_cleaned,\n        'total_dirt': total_dirt,\n        'dirt_remaining': dirt_remaining,\n        'false_positives': false_positives,\n        'false_negatives': false_negatives,\n        'actions_log': actions_log,\n        'final_room_state': room.copy()\n    }\n    \n    return results\n\n# Test imperfect sensor environment\nprint(\"=== Testing Imperfect Dirt Sensor Environment ===\")\nprint(\"Comparing agents with 10% sensor error rate...\\n\")\n\n# Run experiments with imperfect sensor\ndef run_imperfect_sensor_experiments(num_runs=50):\n    results = []\n    \n    agents = [\n        ('Randomized', lambda: simple_randomized_agent),\n        ('Simple Reflex', lambda: simple_reflex_agent),\n        ('Model-Based', lambda: create_model_based_agent())\n    ]\n    \n    for agent_name, agent_factory in agents:\n        print(f\"Testing {agent_name} agent with imperfect sensor...\")\n        \n        for run in range(num_runs):\n            agent = agent_factory()\n            result = vacuum_environment_imperfect_sensor(\n                agent, n=5, p=0.2, sensor_error_rate=0.1, \n                max_steps=200, verbose=False, seed=run\n            )\n            \n            results.append({\n                'agent': agent_name,\n                'run': run,\n                'steps_taken': result['steps_taken'],\n                'success': result['success'],\n                'dirt_cleaned': result['dirt_cleaned'],\n                'total_dirt': result['total_dirt'],\n                'dirt_remaining': result['dirt_remaining'],\n                'false_positives': result['false_positives'],\n                'false_negatives': result['false_negatives']\n            })\n    \n    return pd.DataFrame(results)\n\n# Run experiments\nimperfect_results = run_imperfect_sensor_experiments(50)\n\n# Analyze results\nprint(\"\\n=== Results with Imperfect Dirt Sensor ===\")\nsensor_stats = imperfect_results.groupby('agent').agg({\n    'steps_taken': ['mean', 'std'],\n    'success': 'mean',\n    'dirt_remaining': 'mean',\n    'false_positives': 'mean',\n    'false_negatives': 'mean'\n}).round(2)\n\nsensor_stats.columns = ['avg_steps', 'std_steps', 'success_rate', 'avg_dirt_remaining', 'avg_false_pos', 'avg_false_neg']\nsensor_stats = sensor_stats.reset_index()\nprint(sensor_stats)\n```"
  },
  {
   "cell_type": "code",
   "source": "### 2. Improved Model-Based Agent for Imperfect Sensors\n\n```python\ndef create_robust_model_based_agent(room_size=5, confidence_threshold=0.8):\n    \"\"\"\n    Enhanced model-based agent that handles imperfect dirt sensors using:\n    1. Multiple sensor readings for verification\n    2. Confidence-based decision making\n    3. Re-visiting questionable squares\n    \"\"\"\n    \n    state = {\n        'position': None,\n        'visited': set(),\n        'phase': 'localize',\n        'current_row': 0,\n        'sensor_history': {},  # Track sensor readings per position\n        'clean_confidence': {},  # Confidence level that square is clean\n        'questionable_squares': set(),  # Squares needing re-verification\n        'verification_phase': False\n    }\n    \n    def robust_model_based_agent(bumpers, dirty):\n        \"\"\"Enhanced model-based agent with sensor error handling\"\"\"\n        \n        # Initialize position tracking\n        if state['position'] is None:\n            state['position'] = [0, 0]\n            state['phase'] = 'localize'\n        \n        current_pos = tuple(state['position'])\n        \n        # Record sensor reading\n        if current_pos not in state['sensor_history']:\n            state['sensor_history'][current_pos] = []\n        state['sensor_history'][current_pos].append(dirty)\n        \n        # Calculate confidence in sensor readings for this position\n        readings = state['sensor_history'][current_pos]\n        if len(readings) >= 2:\n            # Multiple readings available - check consistency\n            recent_readings = readings[-3:]  # Use last 3 readings\n            dirt_ratio = sum(recent_readings) / len(recent_readings)\n            confidence = max(abs(dirt_ratio - 1.0), abs(dirt_ratio - 0.0))\n            state['clean_confidence'][current_pos] = confidence\n            \n            # Mark as questionable if readings are inconsistent\n            if confidence < confidence_threshold and len(recent_readings) >= 2:\n                if not all(r == recent_readings[0] for r in recent_readings):\n                    state['questionable_squares'].add(current_pos)\n        \n        # Decision making based on sensor confidence\n        if dirty:\n            # If sensor says dirty, check confidence\n            if current_pos in state['clean_confidence']:\n                confidence = state['clean_confidence'][current_pos]\n                if confidence < confidence_threshold:\n                    # Low confidence - take multiple readings\n                    if len(state['sensor_history'][current_pos]) < 3:\n                        return \"suck\"  # Try cleaning anyway\n                    else:\n                        # Multiple readings taken, decide based on majority\n                        recent_readings = state['sensor_history'][current_pos][-3:]\n                        if sum(recent_readings) >= 2:  # Majority says dirty\n                            return \"suck\"\n            else:\n                return \"suck\"  # First reading or high confidence\n        \n        # Add current position to visited\n        state['visited'].add(current_pos)\n        \n        # Handle different phases\n        if state['phase'] == 'localize':\n            return handle_localization_phase(bumpers)\n        elif state['verification_phase']:\n            return handle_verification_phase(bumpers)\n        elif state['phase'] == 'systematic_clean':\n            return handle_systematic_phase(bumpers)\n        else:\n            # Check if we have questionable squares to revisit\n            if state['questionable_squares'] and len(state['visited']) >= room_size * room_size * 0.8:\n                state['verification_phase'] = True\n                return handle_verification_phase(bumpers)\n            return \"suck\"  # Fallback\n    \n    def handle_localization_phase(bumpers):\n        \"\"\"Navigate to corner for localization\"\"\"\n        if not bumpers['north'] and not bumpers['west']:\n            return \"north\"\n        elif not bumpers['north']:\n            return \"north\"\n        elif not bumpers['west']:\n            return \"west\"\n        else:\n            state['position'] = [0, 0]\n            state['phase'] = 'systematic_clean'\n            state['current_row'] = 0\n            return handle_systematic_phase(bumpers)\n    \n    def handle_verification_phase(bumpers):\n        \"\"\"Revisit questionable squares for additional sensor readings\"\"\"\n        if not state['questionable_squares']:\n            state['verification_phase'] = False\n            state['phase'] = 'done'\n            return \"suck\"\n        \n        # Simple strategy: move randomly and let systematic coverage handle revisiting\n        # In a full implementation, would use path planning to revisit specific squares\n        possible_moves = []\n        if not bumpers[\"north\"]:\n            possible_moves.append(\"north\")\n        if not bumpers[\"south\"]:\n            possible_moves.append(\"south\")\n        if not bumpers[\"east\"]:\n            possible_moves.append(\"east\")\n        if not bumpers[\"west\"]:\n            possible_moves.append(\"west\")\n        \n        if possible_moves:\n            action = np.random.choice(possible_moves)\n            update_position(action)\n            return action\n        return \"suck\"\n    \n    def handle_systematic_phase(bumpers):\n        \"\"\"Systematic serpentine cleaning pattern\"\"\"\n        if state['current_row'] % 2 == 0:  # Even row - go right\n            if not bumpers['east']:\n                update_position('east')\n                return \"east\"\n            else:\n                if not bumpers['south']:\n                    state['current_row'] += 1\n                    update_position('south')\n                    return \"south\"\n                else:\n                    state['phase'] = 'done'\n                    return \"suck\"\n        else:  # Odd row - go left\n            if not bumpers['west']:\n                update_position('west')\n                return \"west\"\n            else:\n                if not bumpers['south']:\n                    state['current_row'] += 1\n                    update_position('south')\n                    return \"south\"\n                else:\n                    state['phase'] = 'done'\n                    return \"suck\"\n    \n    def update_position(action):\n        \"\"\"Update internal position tracking\"\"\"\n        if action == 'north':\n            state['position'][0] -= 1\n        elif action == 'south':\n            state['position'][0] += 1\n        elif action == 'west':\n            state['position'][1] -= 1\n        elif action == 'east':\n            state['position'][1] += 1\n    \n    return robust_model_based_agent\n\n# Test the improved agent\nprint(\"\\n=== Testing Improved Model-Based Agent ===\")\nprint(\"Agent uses sensor confidence and verification strategies...\")\n\ndef run_robust_agent_experiments(num_runs=50):\n    results = []\n    \n    agents = [\n        ('Original Model-Based', lambda: create_model_based_agent()),\n        ('Robust Model-Based', lambda: create_robust_model_based_agent())\n    ]\n    \n    for agent_name, agent_factory in agents:\n        print(f\"Testing {agent_name} with imperfect sensor...\")\n        \n        for run in range(num_runs):\n            agent = agent_factory()\n            result = vacuum_environment_imperfect_sensor(\n                agent, n=5, p=0.2, sensor_error_rate=0.1,\n                max_steps=300, verbose=False, seed=run\n            )\n            \n            results.append({\n                'agent': agent_name,\n                'run': run,\n                'steps_taken': result['steps_taken'],\n                'success': result['success'],\n                'dirt_remaining': result['dirt_remaining'],\n                'false_positives': result['false_positives'],\n                'false_negatives': result['false_negatives']\n            })\n    \n    return pd.DataFrame(results)\n\n# Run comparison\nrobust_results = run_robust_agent_experiments(50)\n\nprint(\"\\n=== Comparison: Original vs Robust Model-Based Agent ===\")\nrobust_stats = robust_results.groupby('agent').agg({\n    'steps_taken': ['mean', 'std'],\n    'success': 'mean',\n    'dirt_remaining': 'mean',\n    'false_positives': 'mean',\n    'false_negatives': 'mean'\n}).round(2)\n\nrobust_stats.columns = ['avg_steps', 'std_steps', 'success_rate', 'avg_dirt_remaining', 'avg_false_pos', 'avg_false_neg']\nrobust_stats = robust_stats.reset_index()\nprint(robust_stats)\n\n# Create visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Success rate comparison\nsuccess_comparison = robust_stats[['agent', 'success_rate']]\nsuccess_comparison.plot(x='agent', y='success_rate', kind='bar', ax=axes[0], rot=45)\naxes[0].set_title('Success Rate Comparison')\naxes[0].set_ylabel('Success Rate')\naxes[0].legend().set_visible(False)\n\n# Steps taken comparison\nsteps_comparison = robust_stats[['agent', 'avg_steps']]\nsteps_comparison.plot(x='agent', y='avg_steps', kind='bar', ax=axes[1], rot=45)\naxes[1].set_title('Average Steps Comparison')\naxes[1].set_ylabel('Average Steps')\naxes[1].legend().set_visible(False)\n\n# Dirt remaining comparison\ndirt_comparison = robust_stats[['agent', 'avg_dirt_remaining']]\ndirt_comparison.plot(x='agent', y='avg_dirt_remaining', kind='bar', ax=axes[2], rot=45)\naxes[2].set_title('Average Dirt Remaining')\naxes[2].set_ylabel('Dirt Squares Remaining')\naxes[2].legend().set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n=== Analysis ===\")\nprint(\"The robust model-based agent implements several strategies to handle sensor errors:\")\nprint(\"1. Multiple sensor readings at each position\")\nprint(\"2. Confidence-based decision making\")\nprint(\"3. Identification and revisiting of questionable squares\")\nprint(\"4. Adaptive cleaning based on sensor consistency\")\nprint(\"\\nThis should result in:\")\nprint(\"- Higher success rate in cleaning all dirt\")\nprint(\"- Better handling of false negatives\")\nprint(\"- Slightly higher step count due to verification overhead\")\n```",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Advanced Implementation (not for credit)\n",
    "\n",
    "If the assignment was to easy for yuo then you can think about the following problems. These problems are challenging and not part of this assignment. We will learn implementation strategies and algorithms useful for these tasks during the rest of the semester.\n",
    "\n",
    "* __Obstacles:__ Change your simulation environment to run experiments for the following problem: Add random obstacle squares that also trigger the bumper sensor. The agent does not know where the obstacles are. Perform experiments to observe how this changes the performance of the three implementations. Describe what would need to be done to perform better with obstacles. Add code if you can. \n",
    "\n",
    "* __Agent for and environment with obstacles:__ Implement an agent for an environment where the agent does not know how large the environment is (we assume it is rectangular), where it starts or where the obstacles are. An option would be to always move to the closest unchecked/uncleaned square (note that this is actually depth-first search).\n",
    "\n",
    "* __Utility-based agent:__ Change the environment for a $5 \\times 5$ room, so each square has a fixed probability of getting dirty again. For the implementation, we give the environment a 2-dimensional array of probabilities. The utility of a state is defined as the number of currently clean squares in the room. Implement a utility-based agent that maximizes the expected utility over one full charge which lasts for 100000 time steps. To do this, the agent needs to learn the probabilities with which different squares get dirty again. This is very tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your ideas/code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}